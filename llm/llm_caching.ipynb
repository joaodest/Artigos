{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8821d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch\n",
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0439980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Manim\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b746e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "def generate_with_timing(model, tokenizer, use_cache, prompt, device, results_queue):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "    \n",
    "    # Armazena o texto gerado\n",
    "    generated_text = []\n",
    "    \n",
    "    # Inicia o timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Thread de gera√ß√£o\n",
    "    thread = threading.Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": tokens,\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"use_cache\": use_cache,\n",
    "            \"streamer\": streamer\n",
    "        }\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    # Consome o streamer\n",
    "    for text in streamer:\n",
    "        generated_text.append(text)\n",
    "    \n",
    "    thread.join()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calcula m√©tricas\n",
    "    elapsed_time = end_time - start_time\n",
    "    full_text = \"\".join(generated_text)\n",
    "    tokens_generated = len(tokenizer.encode(full_text)) - len(tokens[0])\n",
    "    tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n",
    "    \n",
    "    # Armazena resultados\n",
    "    results_queue.put({\n",
    "        \"use_cache\": use_cache,\n",
    "        \"time\": elapsed_time,\n",
    "        \"tokens_generated\": tokens_generated,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"text\": full_text\n",
    "    })\n",
    "\n",
    "# Execu√ß√£o do benchmark\n",
    "def run_benchmark(model, tokenizer, prompt, device):\n",
    "    \"\"\"\n",
    "    Executa o benchmark comparando use_cache=True vs False\n",
    "    \"\"\"\n",
    "    results_queue = Queue()\n",
    "    threads = []\n",
    "    \n",
    "    print(\"Iniciando benchmark...\\n\")\n",
    "    \n",
    "    # Cria e inicia as threads\n",
    "    for use_cache in [True, False]:\n",
    "        thread = threading.Thread(\n",
    "            target=generate_with_timing,\n",
    "            args=(model, tokenizer, use_cache, prompt, device, results_queue)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    # Aguarda todas as threads terminarem\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Coleta resultados\n",
    "    results = []\n",
    "    while not results_queue.empty():\n",
    "        results.append(results_queue.get())\n",
    "    \n",
    "    # Ordena por use_cache para facilitar compara√ß√£o\n",
    "    results.sort(key=lambda x: x[\"use_cache\"], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RESULTADOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n{'COM CACHE' if result['use_cache'] else 'SEM CACHE'}\")\n",
    "        print(f\"  Tempo total: {result['time']:.2f}s\")\n",
    "        print(f\"  Tokens gerados: {result['tokens_generated']}\")\n",
    "        print(f\"  Velocidade: {result['tokens_per_second']:.2f} tokens/s\")\n",
    "        print(f\"  Texto: {result['text'][:100]}...\")\n",
    "    \n",
    "    # Compara√ß√£o\n",
    "    if len(results) == 2:\n",
    "        speedup = results[0]['time'] / results[1]['time'] if results[1]['time'] > 0 else 0\n",
    "        print(f\"SPEEDUP: {speedup:.2f}x \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f1927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando benchmark...\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS DO BENCHMARK\n",
      "======================================================================\n",
      "\n",
      "üî• COM CACHE\n",
      "  Tempo total: 52.11s\n",
      "  Tokens gerados: 100\n",
      "  Velocidade: 1.92 tokens/s\n",
      "  Texto: Black Panther was a great movie. I was really excited to see it. I was really excited to see it. I w...\n",
      "\n",
      "‚ùÑÔ∏è  SEM CACHE\n",
      "  Tempo total: 97.25s\n",
      "  Tokens gerados: 100\n",
      "  Velocidade: 1.03 tokens/s\n",
      "  Texto: Black Panther was a great movie. I was really excited to see it. I was really excited to see it. I w...\n",
      "\n",
      "======================================================================\n",
      "‚ö° SPEEDUP: 0.54x (COM cache √© mais r√°pido)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Black Panther was a\"\n",
    "run_benchmark(model, tokenizer, prompt, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
