{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8821d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch\n",
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0439980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b746e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "def generate_with_timing(model, tokenizer, use_cache, prompt, device, results_queue):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "    \n",
    "    # Armazena o texto gerado\n",
    "    generated_text = []\n",
    "    \n",
    "    # Inicia o timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Thread de geração\n",
    "    thread = threading.Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": tokens,\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"use_cache\": use_cache,\n",
    "            \"streamer\": streamer\n",
    "        }\n",
    "    )\n",
    "    thread.start()\n",
    "    \n",
    "    # Consome o streamer\n",
    "    for text in streamer:\n",
    "        generated_text.append(text)\n",
    "    \n",
    "    thread.join()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calcula métricas\n",
    "    elapsed_time = end_time - start_time\n",
    "    full_text = \"\".join(generated_text)\n",
    "    tokens_generated = len(tokenizer.encode(full_text)) - len(tokens[0])\n",
    "    tokens_per_second = tokens_generated / elapsed_time if elapsed_time > 0 else 0\n",
    "    \n",
    "    # Armazena resultados\n",
    "    results_queue.put({\n",
    "        \"use_cache\": use_cache,\n",
    "        \"time\": elapsed_time,\n",
    "        \"tokens_generated\": tokens_generated,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"text\": full_text\n",
    "    })\n",
    "\n",
    "# Execução do benchmark\n",
    "def run_benchmark(model, tokenizer, prompt, device):\n",
    "    \"\"\"\n",
    "    Executa o benchmark comparando use_cache=True vs False\n",
    "    \"\"\"\n",
    "    results_queue = Queue()\n",
    "    threads = []\n",
    "    \n",
    "    print(\"Iniciando benchmark...\\n\")\n",
    "    \n",
    "    # Cria e inicia as threads\n",
    "    for use_cache in [True, False]:\n",
    "        thread = threading.Thread(\n",
    "            target=generate_with_timing,\n",
    "            args=(model, tokenizer, use_cache, prompt, device, results_queue)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    # Aguarda todas as threads terminarem\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Coleta resultados\n",
    "    results = []\n",
    "    while not results_queue.empty():\n",
    "        results.append(results_queue.get())\n",
    "    \n",
    "    # Ordena por use_cache para facilitar comparação\n",
    "    results.sort(key=lambda x: x[\"use_cache\"], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RESULTADOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n{'COM CACHE' if result['use_cache'] else 'SEM CACHE'}\")\n",
    "        print(f\"  Tempo total: {result['time']:.2f}s\")\n",
    "        print(f\"  Tokens gerados: {result['tokens_generated']}\")\n",
    "        print(f\"  Velocidade: {result['tokens_per_second']:.2f} tokens/s\")\n",
    "        print(f\"  Texto: {result['text'][:100]}...\")\n",
    "    \n",
    "    # Comparação\n",
    "    if len(results) == 2:\n",
    "        speedup = results[0]['time'] / results[1]['time'] if results[1]['time'] > 0 else 0\n",
    "        print(f\"SPEEDUP: {speedup:.2f}x \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Black Panther was a\"\n",
    "run_benchmark(model, tokenizer, prompt, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
